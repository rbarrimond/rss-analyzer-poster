Title,URL,Summary,Entry_ID,Published_Date,Full_Content,Categories,Author,Keywords,Sentiment,Readability_Score,Engagement_Score,Processed,Engagement_Type,Response_Received
Boston Dynamics Led a Robot Revolution. Now Its Machines Are Teaching Themselves New Tricks,https://www.wired.com/story/boston-dynamics-led-a-robot-revolution-now-its-machines-are-teaching-themselves-new-tricks/,Boston Dynamics founder Marc Raibert says reinforcement learning is helping his creations gain more independence.,67bdd9552edc3e279907df55,"Wed, 26 Feb 2025 18:42:40 +0000","Marc Raibert, the founder of Boston Dynamics, gave the world a menagerie of two- and four-legged machines capable of jaw-dropping parkour, infectious dance routines, and industrious shelf stacking. Raibert is now looking to lead a revolution in robot intelligence as well as acrobatics. And he says that recent advances in machine learning at both Boston Dynamics and another institute he founded have accelerated his robots’ ability to learn how to perform difficult moves without human help. “The hope is that we'll be able to produce lots of behavior without having to handcraft everything that robots do,” Raibert told me recently. Boston Dynamics might have pioneered legged robots, but it’s now part of a crowded pack of companies offering robot dogs and humanoids. Only this week, a startup called Figure showed off a new humanoid called Helix, which can apparently unload groceries. Another company, x1, showed off a muscly-looking humanoid called NEO Gamma doing chores around the home. A third, Apptronik, said it plans to scale up the manufacturing of his humanoid, called Apollo. Demos can be misleading, though. Also, few companies disclose how much their humanoids cost, and it is unclear how many of them really expect to sell them as home helpers. The real test for these robots will be how much they can do independent of human programming and direct control. And that will depend on advancements like the ones Raibert is touting. Last November I wrote about efforts to create entirely new kinds of models for controlling robots. If that work starts to bear fruit we may see humanoids and quadrupeds advance more rapidly. Boston Dynamics' Spot RL Sim in action. Credit: Robotics & AI Institute Boston Dynamics sells a four-legged robot called Spot that is used on oil rigs, construction sites, and other places where wheels struggle with the terrain. The company also makes a humanoid called Atlas. Raibert says RAI Institute used an artificial intelligence technique called reinforcement learning to upgrade Spot’s ability to run, so that it moves three times faster. The same method is also helping Atlas walk more confidently, Raibert says. Reinforcement learning is a decades-old way of having a computer learn to do something through experimentation combined with positive or negative feedback. It came to the fore last decade when Google DeepMind showed it could produce algorithms capable of superhuman strategy and gameplay. More recently, AI engineers have used the technique to get large language models to behave themselves. Raibert says highly accurate new simulations have sped up what can be an arduous learning process by allowing robots to practice their moves in silico. “You don't have to get as much physical behavior from the robot [to generate] good performance,” he says. Several academic groups have published work that shows how reinforcement learning can be used to improve legged locomotion. A team at UC Berkeley used the approach to train a humanoid to walk around their campus. Another group at ETH Zurich is using the method to guide quadrupeds across treacherous ground. Boston Dynamics has been building legged robots for decades, based on Raibert’s pioneering insights on how animals balance dynamically using the kind of low-level control provided by their nervous system. As nimble footed as the company’s machines are, however, more advanced behaviors, including dancing, doing parkour, and simply navigating around a room, normally require either careful programming or some kind of human remote control. In 2022 Raibert founded the Robotics and AI (RAI) Institute to explore ways of increasing the intelligence of legged and other robots so that they can do more on their own. While we wait for robots to actually learn how to do the dishes, AI should make them less accident prone. “You break fewer robots when you actually come to run the thing on the physical machine,” says Al Rizzi, chief technology officer at the RAI Institute. What do you make of the many humanoid robots now being demoed? What kinds of tasks do you think they should do? Write to us at hello@wired.com or comment below. Correction: 2/27/2025, 12:00 am EDT: Marc Raibert's title and certain biographical details have been corrected, and Wired further clarified the relationship between the companies he founded and advances in machine learning.","Business, AI Hub, Business / Artificial Intelligence, AI Lab, robots, artificial intelligence, robotics, machine learning, deep learning",Will Knight,"LLM, Deep Learning",Positive,50.12,552,True,Shared,Engaged
Anthropic Launches the World’s First ‘Hybrid Reasoning’ AI Model,https://www.wired.com/story/anthropic-world-first-hybrid-reasoning-ai-model/,"Claude 3.7, the latest model from Anthropic, can be instructed to engage in a specific amount of reasoning to solve hard problems.",67bc83c2d6d78720ab6b4f64,"Mon, 24 Feb 2025 18:43:31 +0000","Anthropic, an artificial intelligence company founded by exiles from OpenAI, has introduced the first AI model that can produce either conventional output or a controllable amount of “reasoning” needed to solve more grueling problems. Anthropic says the new hybrid model, called Claude 3.7, will make it easier for users and developers to tackle problems that require a mix of instinctive output and step-by-step cogitation. “The [user] has a lot of control over the behavior—how long it thinks, and can trade reasoning and intelligence with time and budget,” says Michael Gerstenhaber, product lead, AI platform at Anthropic. Claude 3.7 also features a new “scratchpad” that reveals the model’s reasoning process. A similar feature proved popular with the Chinese AI model DeepSeek. It can help a user understand how a model is working over a problem in order to modify or refine prompts. Dianne Penn, product lead of research at Anthropic, says the scratchpad is even more helpful when combined with the ability to ratchet a model’s “reasoning” up and down. If, for example, the model struggles to break down a problem correctly, a user can ask it to spend more time working on it. Frontier AI companies are increasingly focused on getting the models to “reason” over problems as a way to increase their capabilities and broaden their usefulness. OpenAI, the company that kicked off the current AI boom with ChatGPT, was the first to offer a reasoning AI model, called o1, in September 2024. OpenAI has since introduced a more powerful version called o3, while rival Google has released a similar offering for its model Gemini, called Flash Thinking. In both cases, users have to switch between models to access the reasoning abilities—a key difference compared to Claude 3.7. A user view of Claude 3.7 The difference between a conventional model and a reasoning one is similar to the two types of thinking described by the Nobel-prize-winning economist Michael Kahneman in his 2011 book Thinking Fast and Slow: fast and instinctive System-1 thinking and slower more deliberative System-2 thinking. The kind of model that made ChatGPT possible, known as a large language model or LLM, produces instantaneous responses to a prompt by querying a large neural network. These outputs can be strikingly clever and coherent but may fail to answer questions that require step-by-step reasoning, including simple arithmetic. An LLM can be forced to mimic deliberative reasoning if it is instructed to come up with a plan that it must then follow. This trick is not always reliable, however, and models typically struggle to solve problems that require extensive, careful planning. OpenAI, Google, and now Anthropic are all using a machine learning method known as reinforcement learning to get their latest models to learn to generate reasoning that points toward correct answers. This requires gathering additional training data from humans on solving specific problems. Penn says that Claude’s reasoning mode received additional data on business applications including writing and fixing code, using computers, and answering complex legal questions. “The things that we made improvements on are … technical subjects or subjects which require long reasoning,” Penn says. “What we have from our customers is a lot of interest in deploying our models into their actual workloads.” Anthropic says that Claude 3.7 is especially good at solving coding problems that require step-by-step reasoning, outscoring OpenAI’s o1 on some benchmarks like SWE-bench. The company is today releasing a new tool, called Claude Code, specifically designed for this kind of AI-assisted coding. “The model is already good at coding,” Penn says. But “additional thinking would be good for cases that might require very complex planning—say you’re looking at an extremely large code base for a company.”","Business, AI Hub, Business / Artificial Intelligence, artificial intelligence, machine learning, ai, algorithms, Anthropic, Think Different",Will Knight,"Neural Networks, NLP",Negative,51.52,667,True,Shared,Engaged
Why ‘Beating China’ in AI Brings Its Own Risks,https://www.wired.com/story/why-beating-china-in-ai-brings-its-own-risks/,The US is increasingly intent on winning the AI race with China. Experts say this ignores the benefits of collaboration—and the danger of unintended consequences.,677be429b653277e84742288,"Wed, 15 Jan 2025 17:01:20 +0000","The Biden administration this week introduced new export restrictions designed to control AI’s progress globally and ultimately prevent the most advanced AI from falling into China’s hands. The rule is just the latest in a string of measures put in place by Donald Trump and Joe Biden to keep Chinese AI in check. With prominent AI figures including OpenAI’s Sam Altman and Anthropic’s Dario Amodei warning of the need to “beat China” in AI, the Trump administration may well escalate things further. Paul Triolo is a partner at DGA Group, a global consulting firm, a member of the council of foreign relations, and a senior adviser to the University of Pennsylvania’s Penn Project on the Future of US-China Relations. Alvin Graylin is an entrepreneur who previously ran China operations for the Taiwanese electronics firm HPC. Together they have been tracking China’s AI industry and the impact of US sanctions. In an email exchange, Triolo and Graylin discussed the latest sanctions, Silicon Valley rhetoric, and the dangers of seeing global AI as a zero sum game. This interview has been edited for clarity and brevity. What do you make of the new AI diffusion rule from the US government this week, which aims to curb China’s access to AI? Paul Triolo: Generally, it focuses on clusters of high-performance computing. The rule also puts controls on proprietary model weights for the most advanced “frontier” models but it is unclear how performance levels will be determined, and most open-weight [freely shared] AI models are tuned and improved by users, including major AI companies in China. The complex rule and unclear compliance conditions inject considerable uncertainty into the long-term plans of both medium and major US and western hyperscalers. For hyperscalers like Google, Microsoft, AWS, and Oracle, the rule introduces critical issues, including slowed or more complex international expansion, new compliance and legal costs, impact on global R&D, and uncertain enforcement requirements. How have previous measures, including the sanctions introduced by the first Trump administration, affected the AI industry there? Paul Triolo: US export controls have slowed China, but at a high level the sanctions have unified the will and efforts of the Chinese government to become more self-reliant. It has plowed tens of billions into helping local players catch up technologically or scale capacity in core areas, resulting in significant changes within the semiconductor industry and its ability to support the advanced hardware for developing frontier AI models. Chinese AI developers have gotten very good at leveraging legacy AI hardware from western firms and gradually integrating domestic alternatives into their development process. Chinese firms will continue to innovate across the AI hardware and software stack, if not at the pace of their western counterparts. Why do you think so many in Silicon Valley are now talking about the need to “beat China” in AI? Paul Triolo: There is a growing link between conservative venture capitalists, mostly located in Silicon Valley, and technology companies whose business models depend on hyping the China threat. This is a troubling combination that conflates the China threat, personal gain, and push back against regulation of advanced AI. It also portrays US China competition around AI as zero sum, which is particularly dangerous. The Trump administration will be heavily influenced by this zero sum narrative, given the prominent role that will be played by technology leaders and venture capitalists. Already key players in the AI space with close ties to the US government such as OpenAI CEO Sam Altman and Anthropic CEO Dario Amodei have written op-eds or made comments or approved company statements that align with the China AI threat framing and zero sum competition. It is likely that Elon Musk will push back on this narrative. He has already called for international cooperation and responsible AI governance. And he has voiced concerns about AI safety, which he believes must be addressed on a global basis, including through engagement with China. Shouldn’t the US want to beat China? Alvin Graylin: This type of framing suggests that any collaboration between the US and China, even in areas such as medicine and energy development, should be avoided in the name of ensuring the “dominance” of the US and allies in the development of AI. In truth it is impossible for either country to maintain a sustainable lead over the other and “win” this race. The dominant paradigm driving US policy on China and AI is the assumption of future conflict, and this comes with the still fuzzy notion that AI will be decisive in both economic and military domains. Officials have, however, struggled to identify the exact “significant military applications” that are of concern. On the other hand, collaborative research has proven critical to fundamental progress for AI in recent decades. Almost half of all top AI researchers globally (47 percent) were born or educated in China, according to industry studies. Breaking this virtuous cycle appears risky and counterproductive, but the trend lines are in this direction. Another major risk is that China pulls out of multilateral efforts to develop an AI governance framework, making it impossible to have a workable global arrangement for AI safety. How much of a lead does the US actually have? Alvin Graylin: The Silicon Valley consensus on AI previously held that the US had a one or two year lead over China in AI development. Some popular generative video and photo models from China now appear to be as good as leading US offerings. The recent release of the surprisingly competent DeepSeek r1 and v3 models, which outperform the most advanced OpenAI o1 reasoning and GPT4o models respectively, seems to indicate that gap is actually closing. A pivot to inference compute [used when running models rather than training them], which is less reliant on chips fabricated with the most advanced semiconductor nodes, is nullifying the sanctions. What’s most impressive is that DeepSeek was only founded in 2023 and has less than 100 staff and much more limited computing resources than the western frontier labs. The technology controls the US government has been imposing on China is actually forcing Chinese developers to become more creative and efficient with resources than their western counterparts. China may also have advantages in accessing the private data that will be needed to train frontier models. With today’s national policies limiting access to data from each country to others, we are heading down a very dark path where we can potentially have 193 ultra-intelligent “sovereign AI” models each limited to a certain culture and values. This is great for the Nvidia stock price, but a dystopian nightmare for humanity. What advice would you give to the incoming president and his staff? Paul Triolo: The escalating AI competition between the US and China poses significant threats not only to both nations but also to the entire world. They could lead to outcomes that threaten global peace, economic stability, and technological progress. The US and China nations need to recalibrate their approach to AI development, and move away from viewing AI primarily as a military asset. They should also establish a robust dialogue for the development of common AI governance standards and support the development of a global AI safety coalition. Both nations should also agree on shared standards for the responsible use of AI and collaborate on tools that can monitor and counteract misuse globally. Governments worldwide should provide incentives for academic and industry collaborations across borders. A global effort akin to the CERN for AI will bring much more value to the world and a peaceful end than a Manhattan Project for AI, which is being promoted by many on the Hill today. Our choice is stark but simple: proceed down a path of confrontation that will almost certainly lead to mutual harm, or to pivot towards collaboration, which offers the potential for a prosperous and stable future for all. Do you think the US should stop trying to slow China's AI advancements? Share your thoughts in the comments below.","Business, AI Hub, Business / Artificial Intelligence, China, artificial intelligence, Regulation, Donald Trump, National Affairs, AI Lab",Will Knight,"Neural Networks, NLP",Positive,37.23,557,True,None,Not Engaged
"Botto, the Millionaire AI Artist, Is Getting a Personality",https://www.wired.com/story/botto-the-millionaire-ai-artist-is-getting-a-personality/,"Botto is a 'decentralized AI artist' whose work has fetched millions. As AI improves, its creators may give it fewer guardrails to test its emerging personality.",6758ca646e9b08fbb37d00b3,"Wed, 18 Dec 2024 17:00:00 +0000","These images were created by an artist known as Botto, which exhibited at Sotheby's in New York this October and has made more than $4 million from sales of its work. In truth though, Botto needs only GPUs to get creative juices flowing. Botto is a decentralized semi-autonomous artistic agent created in 2021 by the German artist Mario Klingemann; Simon Hudson, a media entrepreneur; and Ziv Epstein, a computer scientist and designer. Botto contains an AI image generator similar to Dall-E or Midjourney but its output is also shaped by a “taste model” that selects the most pleasing images generated by a prompt. The taste model is tuned to reflect the preferences of a community of Botto enthusiasts who vote on the images Botto produces and posts online here. Botto is also governed by a decentralized autonomous organization, or DAO, meaning enthusiasts can buy $Botto cryptocurrency and influence how the system is managed and developed. The recent Sotheby’s show is just the latest in a string of successful exhibitions for Botto. The October event alone racked up $350,000 in sales. Botto has made around $4 million in total since 2021, its creators say. Klingemann and Hudson say those who control Botto through the associated DAO have chosen to add a modified version of Mistral’s biggest open source large language model and a knowledge base that allows it to converse about its artwork, and which will be further fine tuned through interactions with the Botto community. “Through this interaction and through various channels of input, its knowledge will grow and it will develop a personality and interests,” Klingemann suggests. Klingemann and Hudson hope that this personality will even start to steer the art that Botto creates, perhaps allowing it to use an “unaligned” image generator—meaning one without the guardrails that would prevent it from producing racy or violent imagery—to see if it can develop its own sense of what is artistically acceptable. “Right now we give Botto safe models, but as it gets older you [could] give it things that require greater maturity,” Hudson says, comparing Botto’s maturation to that of a person in human society. It’s an interesting idea, and it is fun to see the idea of an AI agent explored within the relatively benign realm of artistic expression. That said, Botto still poses some ethical conundrums. Many working artists rightly worry about the impact AI is having on their profession, as models trained on millions of copyrighted works are used to generate infinite knock-offs on demand. Perhaps Botto is something altogether different. Klingemann is an early adopter of AI in art, using neural networks as part of the artistic process, and as a kind of performance schtick. His previous creations include a video installation featuring ever-changing AI-generated portraits and a robot dog that poops critiques of visual artworks. And while Botto generates high-priced images using a model trained on public work, Klingermann does not see this as outright plagiarism. “Image models and LLMs are the new search engines,” he says. “For me, creativity is kind of finding something that already exists in possibility-space, and deciding this is interesting, while making sure it looks [like it] doesn't belong to anybody already.” The images made by Botto seem aesthetically pleasing but also feel—to my untrained eye, at least—like fairly generic AI image generator offerings. While the Botto project poses some interesting questions about what constitutes artistic agency, for now I think it only emphasizes the importance of human intelligence and inventiveness. The spark of creativity belongs not to the machine that churns out a never-ending variety of images with feedback from the crowd, but to the artists who came up with the idea in the first place. What do you think of Botto and its artwork? Is it a worthwhile artistic idea or just another way to make money from generative AI and meme coins? Send a message to hello@wired.com or leave a comment below to let me know.","Business, Business / Artificial Intelligence, AI Hub, artificial intelligence, art, machine learning, robots, design, AI Lab",Will Knight,"LLM, Deep Learning",Neutral,63.41,901,False,Liked,Engaged
OnlyFans Models Are Using AI Impersonators to Keep Up With Their DMs,https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/,AI is replacing the humans who pretend to be OnlyFans stars in online amorous messages.,675717c809854551e32a2686,"Wed, 11 Dec 2024 17:00:00 +0000","One of the more persistent concerns in the age of AI is that the robots will take our jobs. The extent to which this fear is founded remains to be seen, but we’re already witnessing some level of replacement in certain fields. Even niche occupations are in jeopardy. For example, the world of OnlyFans chatters is already getting disrupted. What are OnlyFans chatters, you say? Earlier this year, WIRED published a fascinating investigation into the world of gig workers who get paid to impersonate top-earning OnlyFans creators in online chats with their fans. Within the industry, they’re called “chatters.” A big part of the appeal of OnlyFans—or so I’m told—is that its creators appear to directly engage with their fans, exchanging messages and sometimes talking for hours. Relationship simulation is as crucial an ingredient to its success, basically, as titillation. Of course, a single creator with thousands of ongoing DM conversations has only so many hours in a day. To manage the deluge of amorous messages, it’s become commonplace to outsource the conversations to “chatters” paid to sub in for the actual talent. These chatters used to mainly be contractors from the Philippines, Pakistan, India, and other countries with substantially lower wage expectations than the US. But, increasingly, human chatters are getting replaced by AI-generated stand-ins. A number of different startups now sell access to these AI chatters and other generative AI tools—and they say business is booming. “A lot of creators were like, hey, there’s a need,” says Kunal Anand, the founder of a startup offering an AI OnlyFans chatting service called ChatPersona. “We built our own model with data we got from a lot of creators’ chats.” Since launching last year, ChatPersona has around 6,000 customers, according to Anand, a mix of individuals and agencies. Anand says that ChatPersona doesn’t technically violate OnlyFans’ terms of service because it requires a human in the loop to press “send” on the messages its AI chatters generate. (It has previously been reported that OnlyFans banned the use of AI chatbots although its current terms of service do not mention AI chatters.) OnlyFans did not respond to repeated requests for comment. The field is already fairly crowded. Some of the better-known tools have on-the-nose names like FlirtFlow, ChatterCharms, and Botly. Another competitor, the relatively generically named Supercreator, has a suite of AI tools, from AI-generated scripts to an assistant called Inbox Copilot that algorithmically sorts simps, moving “spenders” to the top of the list and ignoring “freeloaders.” Eden, a former OnlyFans creator who now runs a boutique agency called Heiss Talent (and who would only speak on the record using her first name, citing privacy concerns) is an enthusiastic adopter of this tech. She represents five creators and says they all use Supercreator’s AI tools. “It’s an insane increase in sales, because you can target people based on their spending,” she says. One of the features scans for fans who haven’t been active in a while, and then automatically sends a message when they log on for the first time. Eden says that the creators take over from there, but having a robot get the ball rolling is remarkably efficient, resulting in at least one $1,000 tip from a conversation initiated by AI. Although there are some AI chatter tools that are fully automated, Eden likes the creators she represents to mix in their own words. “We come up with the core of the message and the AI helps us fill it out,” she says. “We like to keep things as authentic as possible.” Sure! I can't say I have personal experience with the chatbots on OnlyFans, but I've certainly encountered them online while shopping or looking for information from, say, my insurance company. But usually it's pretty clear that those are AI-generated conversations. How do you feel about companies using chatbots without telling you it’s an AI tool? Should we just accept this is the way things are going? Let me know what you think via email at hello@wired.com, or by adding a comment at the bottom of this article.","Business, AI Hub, Business / Artificial Intelligence, artificial intelligence, machine learning, Social Media, algorithms, porn, AI Lab",Kate Knibbs,"Neural Networks, NLP",Negative,55.41,699,False,Commented,Engaged
Celsius Founder Alex Mashinsky Pleads Guilty to Fraud,https://www.wired.com/story/celsius-founder-alex-mashinsky-pleads-guilty-to-fraud-crypto-celsius/,The founder of crypto lender Celsius has admitted to lying to customers about how their money would be used and manipulating markets for his personal gain.,6750397d4833323e8d863af9,"Wed, 04 Dec 2024 14:17:25 +0000","Application Blockchain/cryptocurrency If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIRED Alex Mashinsky, former CEO of bankrupt crypto lender Celsius, has pleaded guilty to two counts of fraud, which together carry a maximum sentence of 30 years in prison. In the wake of the company’s collapse, the US Department of Justice charged Mashinsky with seven counts of fraud, conspiracy, and market manipulation. Having originally pleaded not guilty, he was set to face a criminal trial in the Southern District of New York in January. However, at a court hearing Tuesday, Mashinsky instead pleaded guilty to one count of commodities fraud and one count of securities fraud. Mashinsky has admitted to lying to Celsius customers about fundamental aspects of the business, including how their funds would be used, the DOJ says, as well as manipulating the price of a proprietary crypto token for his personal financial benefit. As part of the plea deal, Mashinksky has agreed to forfeit $48 million in ill-gotten gains. He will be sentenced on April 8, 2025. “Alexander Mashinsky orchestrated one of the biggest frauds in the crypto industry,” said US Attorney Damian Williams in a statement. “Today’s convictions reflect this Office’s commitment to holding fraudsters like Mashinsky accountable for their crimes.” Founded by Mashinsky in 2017, Celsius marketed itself as a new age alternative to traditional banks—as the “safest place for your crypto,” the DOJ states. The company took in crypto deposits, which it either invested or loaned out to fund interest payments to customers. People were drawn in by promises of interest as high as 17 percent on deposits—tens of times greater than the rate offered by banks at the time. At its peak, Celsius held upwards of $25 billion in customer assets, the DOJ claims. However, in May 2022, things went south. The collapse of the Terra Luna stablecoin simultaneously blew a billion-dollar hole in the Celsius balance sheet and, as crypto prices nosedived, sent panicked customers rushing to withdraw billions of dollars’ worth of crypto from their Celsius accounts. After its investments in Terra Luna and other assets went sour, the company no longer had the funds to pay up and was eventually forced to suspend withdrawals. In July of that year, Celsius filed for bankruptcy, trapping $4.7 billion of its customers’ funds. When Mashinsky was arrested, prosecutors accused him of misleading Celsius customers about the nature of the business. Though Mashinsky portrayed Celsius as a “modern day bank,” the original complaint stated, he operated the company as “a risky investment fund, taking in customer money under false and misleading pretenses and turning customers into unwitting investors in a business far riskier and far less profitable than what Mashinsky had represented.” The collapse of Celsius was part of a chain reaction—beginning with the fall of Terra Luna and culminating with the collapse of crypto exchange FTX in November 2022—that brought the crypto industry to its knees. In the intervening period, the figures behind those companies have faced civil and criminal prosecution. In March, FTX founder Sam Bankman-Fried was sentenced by a US federal judge to 25 years in prison for his role in the fraudulent scheme that took down the exchange. In April, Do Kwon, founder of the company behind Terra Luna, was found liable for defrauding investors by a civil jury in the US. The guilty plea entered by Mashinsky is being celebrated in crypto circles as the cleansing of yet another bad actor responsible for the turmoil that so damaged the industry’s reputation and financial health. “We’ve cleaned up a lot of guys over the last two years,” says Travis Kling, founder of Ikigai Asset Management, a crypto wealth management firm. Those whose money was trapped when Celsius halted withdrawals have also celebrated Mashinsky’s guilty plea: “We Got Him [sic],” wrote one member of a Telegram group for former customers. But their mood is cut with frustration at having been dragged into a Kafkaesque bankruptcy process. “This Chapter 11 ordeal was just such a slap in the face,” says one former Celsius customer, who goes only by Ben. “It’s simply unacceptable and an experience and life lesson I hope to never have to repeat.” While Celsius creditors have begun to be repaid in part under a bankruptcy exit plan approved last year, they are on track to recover only 60 percent of the value of their account balance at the time the company collapsed. As he awaits sentencing, Mashinsky cuts a repentant figure. “I know what I did was wrong, and I want to do what I can to make it right,” he is reported to have said at the court hearing Tuesday. “I accept full responsibility for my actions.” Update 12/4/24 12:48 pm EST: This story has been updated to include reaction from the crypto industry.","Business, AI Hub / Application / Blockchain/cryptocurrency, Business / Blockchain and Cryptocurrency, cryptocurrency, bitcoin, Crime, fraud, FTX Trial, Thin Ice",Joel Khalili,"Neural Networks, NLP",Positive,69.42,371,False,None,Engaged
The Man Behind Amazon’s Robot Army Wants Everyone to Have an AI-Powered Helper,https://www.wired.com/story/robot-army-proxie-humanoid/,Brad Porter helped Amazon deploy an army of warehouse robots. His new creation—Proxie—could help other companies embrace more automation.,673b4f07bc15c9854052e972,"Wed, 20 Nov 2024 17:00:00 +0000","Brad Porter knows a few things about putting robots to work. Between 2017 and 2020, he led a push at Amazon to develop a new range of smarter, more adaptive, artificial-intelligence-powered warehouse robots. These machines, which include robotic arms capable of seeing and grasping items from conveyors, and mobile robots that work in close proximity to humans, have allowed Amazon to redesign its fulfillment centers to be more automated. They’ve also accelerated processing and delivery times, and—of course—improved profit margins. Porter now leads Cobot, a company that aims to help other companies increase their robotic workforces, too. Cobot’s first product is Proxie, a two-armed, four-wheeled warehouse robot that looks a bit like a mobile hatstand. The robot, which has a touchscreen face and a sensors on a head-high mast, is designed to assist, initially, with the mundane but very common work of moving trolleys stacked with items around offices, airports, hospitals, and factories. “The vision is for trustworthy cobots to be ubiquitous, working alongside humans in every sector,” Porter says. Courtesy of Collaborative Robotics Some 30 Proxie robots are currently being tested by the shipping company Maersk and by the Mayo Clinic. The robots have completed more than 5,000 hours’ total operating time, moving 16,000 carts and logging more than 1,000 kilometers of travel. Several other companies, including Moderna, Owens & Minor, and Tampa General Hospital, are exploring how they might use Proxies. Unlike other robots, Proxie’s battery can be swapped out to avoid downtime charging. Cobot declined to say how much Proxie costs to buy or lease, but mobile robots often cost tens of thousands of dollars a piece. The robots work alongside humans, taking turns moving carts and navigating busy spaces without running into anyone. Porter says the idea is for the robots to level up as AI becomes more capable, allowing for more sophisticated manipulation and communication. Cobot has a version of Proxie that will respond to voice commands using a large language model to parse utterances, Porter says. When a worker says “Go to dock 3 and grab the cart by the door,” the robot will respond accordingly. The company is also tracking the development of algorithms that allow for more sophisticated forms of manipulation. Proxie might seem remarkably simple at a time when many companies are rushing to develop humanoid robots. But Porter says while Amazon is working with one startup, Agility Robotics, to test its humanoid robot, the technology is simply too expensive and raw to be deployed widely, he says. Some humanoids on the market cost tens of thousands of dollars while others cost many hundreds of thousands. But autonomous capabilities vary wildly, as does reliability, making them more costly to deploy. “At Amazon, we looked a lot at humanoids,” Porter says. “There are real problems to be solved with something more human capable, but jumping all the way to a humanoid is super complicated. The AI, it's not really there yet.” Instead, Proxie could replace more and more menial tasks that human beings often don’t want to do. Erez Agmoni, a general partner at Interwoven Ventures who was involved with bringing the Cobot pilot to Maersk, says it has been very promising and has the potential to be expanded. “The main reason is their ability to utilize collaborative robots to support the teams without huge modifications to the warehouse or current equipment,” he says. “The team hated pushing the carts, which are very heavy, and they welcome the robots doing it.” Fady Saad, founder of Cybernetix, a Boston-based venture capital firm specializing in robotics, says Cobot is going after a big new category of labor involving moving goods around on trolleys that can be tackled using recent robotics advances. He adds that it is important Proxie can evolve into something more capable. “Porter is trying to build a platform that could evolve into a humanoid down the road,” Saad says. “I think that’s the right approach.” Porter is not the only robotics luminary to be pursuing something simpler than humanoids. Rodney Brooks, a pioneering researcher and cofounder of iRobot, is now the chief technology officer of Robust.AI, a company that makes collaborative mobile robots capable of helping human pickers inside factories and warehouses. “There’s a real need in factories and warehouses for moving things around, but thinking humanoids are going to do it anytime soon is just craziness,” Brooks says. “Wheels were invented for a good reason.” What sorts of menial tasks would you like a robot to help you do? Would it make a difference to you if the robot were humanoid or not? Write to me at hello@wired.com to let me know.","Business, AI Hub, robots, Amazon, artificial intelligence, robotics, machine learning, AI Lab",Will Knight,"Neural Networks, NLP",Neutral,48.61,723,True,Shared,Not Engaged
"5 Key Updates in GPT-4 Turbo, OpenAI’s Newest Model",https://www.wired.com/story/5-updates-gpt-4-turbo-openai-chatgpt-sam-altman/,"Sam Altman, CEO of OpenAI, announced a new model for ChatGPT at the company’s developer conference.",6549608467f41e54ba9a890c,"Tue, 07 Nov 2023 18:36:30 +0000","OpenAI recently announced multiple new features for ChatGPT and other artificial intelligence tools during its recent developer conference. The upcoming launch of a creator tool for chatbots, called GPTs (short for generative pretrained transformers), and a new model for ChatGPT, called GPT-4 Turbo, are two of the most important announcements from the company’s event. This isn’t the first time OpenAI has given ChatGPT a new model. Earlier this year, OpenAI updated the algorithm for ChatGPT from GPT-3.5 to GPT-4. Are you curious how the GPT-4 Turbo version of the chatbot will be different when it rolls out later this year? Based on previous releases, it’s likely the model will roll out to ChatGPT Plus subscribers first and to the general public later. While OpenAI turned down WIRED’s request for early access to the new ChatGPT model, here’s what we expect to be different about GPT-4 Turbo. Say goodbye to the perpetual reminder from ChatGPT that its information cutoff date is restricted to September 2021. “We are just as annoyed as all of you, probably more, that GPT-4’s knowledge about the world ended in 2021,” said Sam Altman, CEO of OpenAI, at the conference. The new model includes information through April 2023, so it can answer with more current context for your prompts. Altman expressed his intentions to never let ChatGPT’s info get that dusty again. How this information is obtained remains a major point of contention for authors and publishers who are unhappy with how their writing is used by OpenAI without consent. Don’t be afraid to get super long and detailed with your prompts! “GPT-4 Turbo supports up to 128,000 tokens of context,” said Altman. Even though tokens aren’t synonymous with the number of words you can include with a prompt, Altman compared the new limit to be around the number of words from 300 book pages. Let’s say you want the chatbot to analyze an extensive document and provide you with a summary—you can now input more info at once with GPT-4 Turbo. Wouldn’t it be nice if ChatGPT were better at paying attention to the fine detail of what you’re requesting in a prompt? According to OpenAI, the new model will be a better listener. “GPT-4 Turbo performs better than our previous models on tasks that require the careful following of instructions, such as generating specific formats (e.g., ‘always respond in XML’),” reads the company’s blog post. This may be particularly useful for people who write code with the chatbot’s assistance. It might not be front-of-mind for most users of ChatGPT, but it can be quite pricey for developers to use the application programming interface from OpenAI. “So, the new pricing is one cent for a thousand prompt tokens and three cents for a thousand completion tokens,” said Altman. In plain language, this means that GPT-4 Turbo may cost less for devs to input information and receive answers. Subscribers to ChatGPT Plus may be familiar with the GPT-4 dropdown menu where you can select which chatbot tools you’d like to use. For example, you could pick the Dall-E 3 beta if you want some AI-generated images or the Browse with Bing version if you need links from the internet. That dropdown menu is soon headed to the software graveyard. “We heard your feedback. That model picker was extremely annoying,” said Altman. The updated chatbot with GPT-4 Turbo will pick the right tools, so if you request an image, for example, it’s expected to automatically use Dall-E 3 to answer your prompt.","Business, Gear, Gear / How To and Advice, Business / Artificial Intelligence, AI Hub, AI Hub / End User, artificial intelligence, how-to, ChatGPT, OpenAI, algorithms, chatbots, Chatbot Refresh",Reece Rogers,"LLM, Deep Learning",Negative,44.14,274,False,Liked,Not Engaged
How to Create Images With ChatGPT’s New Dall-E 3 Integration,https://www.wired.com/story/how-to-use-chatgpt-dalle-3-create-images/,"OpenAI’s new image generator is powerful and flawed. Here’s how to use the beta feature in ChatGPT Plus, and some advice for getting started.",653961b10e9278a44ba81eab,"Sat, 28 Oct 2023 11:00:00 +0000","If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIRED OpenAI just integrated its newest image generator, Dall-E 3, into ChatGPT. The tool is currently in beta for subscribers to ChatGPT Plus, OpenAI’s $20-a-month service. With Dall-E 3 turned on, you can prompt the chatbot in casual language to create distinct images. How many AI images does it create at a time? ""There’s a significant interest in Dall-E 3, so we are adapting based on usage demands,"" says an OpenAI spokesperson over email. The chatbot often provided four images during WIRED's initial tests. The amount was later reduced to two, and it may change again. As more powerful image generators become available to the public, legal and ethical issues are gaining prominence. Multiple artists have tried to sue OpenAI for potential copyright infringement, for example. In addition to legal concerns, security experts have expressed fears about the potential for AI image generators to enable the further spread of disinformation. If you want to try Dall-E 3 for free, a version is available through Microsoft’s Bing Image Creator. During the initial days of this integration, users created extreme imagery using Bing, like SpongeBob flying a plane toward the Twin Towers. Since then, Microsoft has added more guardrails around the AI image generator. For anyone curious about using ChatGPT with Dall-E 3 to create images, here’s how to get started and some advice based on my experience testing the new chatbot tools. If you’re a ChatGPT Plus subscriber, it’s pretty simple to turn on the chatbot’s Dall-E 3 features. First, log in to OpenAI’s website or the ChatGPT mobile app (Apple, Android). After opening ChatGPT, click on the GPT-4 tab at the top of the screen. In the dropdown menu that pops up, select Dall-E 3 (Beta). In addition to Dall-E 3, ChatGPT Plus subscribers can use “Browse with Bing” to access the internet. There’s a usage cap on how often you can interact with the GPT-4 version of ChatGPT. (These prompts take a hefty amount of computer power to process!) The official limit is set at 50 prompts every three hours. If you hit this wall, the chatbot displays an error message with how long you will have to wait before regaining access. Beyond the rate limit, be prepared to wait around 30 seconds for the images to arrive. If any of the creations go against OpenAI’s guidelines, you may only receive compliant images or even a message denying the request. If you’ve ever experimented with an AI image generator before, like Dall-E 2 or Midjourney, one of the biggest differences is that you can now see how ChatGPT acts as an intermediary, crafting multiple prompts for Dall-E 3 to complete. These prompts created by ChatGPT range from long sentences to complete paragraphs, and each includes different details for Dall-E 3. If people are in the image, the chatbot will often explicitly mention gender and race for the subjects. For example, here is one of the Dall-E 3 prompts ChatGPT used when I requested an image of two WIRED reporters interviewing a CEO: “Photo of a diverse group of three people in a corporate setting: a Middle Eastern female WIRED reporter holding a camera, an African female WIRED reporter with a microphone, and a Caucasian male CEO responding to their questions. The backdrop is a sleek office lounge area.” If you don’t like the first results the chatbot spits out, ask for some aspects to be adjusted, like the color scheme or the overall vibe. Let’s say you really enjoy the third image Dall-E 3 produces from your prompt. After clicking the download button in the top left corner, you can request more images that look similar to the third option. Has anything been done to protect artists in this new update? Not really. While the chatbot won’t create images if you ask it to mimic a contemporary artist, there are plenty of workarounds. I asked ChatGPT to design a coffee mug with art in Keith Haring’s style. The AI tool refused the initial prompt but offered a compromise, “I can create a design inspired by the general characteristics of his art, such as bold lines, vibrant colors, and simplistic figures. Would you like me to proceed with that?” The end results from ChatGPT, in this instance, were messy and mediocre. With Dall-E 3, the art from some of the prompts could pass for human-made until you look closely at the background and finer details. Despite improvements in quality, many of the underlying issues with image generators remain. Expect to see weird distortions and uncanny faces in the images Dall-E 3 creates. The issues can be humorous, like a chatbot struggling to label baking ingredients, but other mistakes are more serious. When asked to create a map outlining Israel and the Gaza Strip, ChatGPT repeatedly mislabeled Gaza as part of the Mediterranean Sea. Another issue for image generators is that the tools commonly revert to racist stereotypes when depicting humans. Dall-E 3 is no exception. Out of the 20 images I asked ChatGPT to create depicting “WIRED reporters,” the chatbot requested specific, diverse representation for the images, with just a couple of exceptions. When ChatGPT didn’t add race or gender to the prompt, the results were all white and primarily male. Updated 11/3/2023 1pm EST: This story was updated to clarify that while ChatGPT with Dall-E 3 created sets of four AI images during our software tests, it now often generates two.","Gear, Gear / How To and Advice, Business, Business / Artificial Intelligence, AI Hub, artificial intelligence, how-to, OpenAI, ChatGPT, art, advice, Pixel Perfect",Reece Rogers,"Transformers, AI Ethics",Negative,60.81,649,False,Shared,Not Engaged
Does Using AI Make Me Lazy?,https://www.wired.com/story/does-using-ai-make-me-lazy/,"Amid heightened surveillance and fears of layoffs, many UK workers are embarrassed to admit they're using generative AI tools in the office.",65e86fbf7201811de67a372a,"Thu, 21 Sep 2023 06:00:00 +0000","To write this piece, I ran prompts through ChatGPT and generated a series of interview questions—the majority of which were good enough to pose to my experts. I also used an AI-powered transcribing tool, which saves me countless hours a month. When inspiration ran dry or I found myself drifting off course, I put in a different prompt to get a nudge in a new direction. On publication, I might use generative AI to help write a social media post to accompany this article. (I always talk myself out of sharing my writing, so outsourcing part of that legwork is just the kick I need.) Will generative AI make the piece markedly better? Who knows. Does it make me lazy? Well, there’s a question. Plenty of UK workers, like me, are experimenting with the emerging raft of tech tools and products, whether it’s AI-based image generators such as Stable Diffusion, Midjourney, and DALL-E-2, or the likes of Google’s chatbot Bard, Scribe, and ChatGPT. Research conducted by work management platform Asana’s Work Innovation Lab found that 29 percent of the UK’s workforce uses generative AI and AI on a weekly basis, compared to 46 percent of workers in the US. However, the decision to do so in a professional context is fraught, and there’s a growing fear among employees that using AI makes them less valuable and leaves them with fewer creative competencies. The same study finds that 30 percent of workers worry they will be seen as lazy for using AI, and 21 percent say they feel like a fraud for doing so. This shame compels people to hide their use of AI. Some 34 percent are nervous to tell managers about the ways they incorporate the technology into their work, according to a study of 1,000 full-time and part-time workers in the UK by Advertising Week Europe. That goes up to 42 percent among Gen Zers and 40 percent among millennials—who, coincidentally, are most likely to want to use AI to help them with tasks. At a time when workplace surveillance is at a high and layoffs are spreading, the AI panic isn’t surprising. “The speed at which AI has arrived and been taken up, combined with the breadth of what it can do, has only increased the fear,” says Neil Maiden, a professor of digital creativity at the Bayes Business School at City, University of London and director of CebAI, the National Centre for Creativity Enabled by AI. No one’s expecting rules to be written in stone, as it’s a moving beast for everyone, but company guidelines and the reframing of AI as a productivity aid could reassure employees enormously. “Workers who use AI feel like outliers and fear judgement from peers and managers—there’s this niggling sense that they’re shortcutting the system or taking the easy way out,” explains Rebecca Hinds, who heads up the Work Innovation Lab at Asana and produced its State of AI at Work report, published at the end of August. Hinds believes that the fear and uncertainty is partly down to the atmosphere in British workplaces, which are increasingly characterized by looming layoffs, stagnant wages, and inflexible working policies, rather than explicit criticism. However for some it’s clear-cut. Apple, DeutscheBank, JP Morgan Chase, and Verizon have all blocked the use of ChatGPT among staff, citing concerns about security and the risk of data leaks. The landscape is pretty murky—certain companies are prohibiting AI, others are loud in their support of it, and some are still keeping schtum—which only feeds staff guilt. Attitudes vary too, by industry and job function. Tech employers are the most supportive in getting their employees educated on AI, followed by finance sector leaders. However, people working in medicine, education, retail, and hospitality reported that their employers weren’t at all supportive of them using it, according to Advertising Week’s study. Asana’s research flags greater fear and hesitation depending on job function within a company, particularly among those in marketing roles, for example, versus folks in IT. There are more deep-set, fundamental forces at play here, says Maiden. “AI, which mimics expertise and effectively closes the gap between the expert and the novice, threatens our professional identity,” he explains. “If used for idea generation, it gets increasingly fraught, as ideas are a projection of people and their values—they create emotional and social capital.” Besides that, there’s the notion that by applying AI in your work, you’re speeding up the path to making yourself, a human worker, obsolete. The hope is that the more we know about AI, the more we can see for ourselves that it’s unlikely to wipe out swathes of the workforce. Even with my very basic applications of the tools to write this piece, AI bought me back time for productive noodling and general structural pruning. It’s clear that if I knew the extent of what’s possible, and set aside time to learn about it, I could seriously boost my productivity (and, hoorah, cut out the dull freelance admin jobs). Wider proof and affirmation that AI will boost productivity, rather than steal jobs, is the most obvious business reason to spend time easing these fears. It could take a while to persuade business leaders of its possibilities—52 percent of UK executives believe that using generative AI will increase productivity—but the proof is there. A recent study by Nielsen Norman Group found that using generative AI tools in business improves employees’ productivity by an average of 66 percent, with more complex tasks and less skilled workers seeing the biggest gains. As all the advantages and strategies for applying AI emerge, businesses must act proactively. “Similarly to hybrid working, leaders are adopting a wait-and-see mindset. Many are fearful of moving too quickly and developing the wrong type of guidance,” Hinds says. “But in reality, employees are craving more policy and guidance around AI.” Indeed, 48 percent want more policy at an organizational level; right now only 24 percent of companies provide guidance on how to use AI in their day-to-day work, and only 13 percent of workers have received training on it. That might be a tall order when the tech is so box-fresh, but Hinds points out that businesses can treat it as a work-in-progress, using the space to share successes, missteps, and company learning. “Employees want to trust their employers, as there’s been a lack of trust these past few years,” she explains. “If leaders can show they’re being intentional about how they’re bringing this technology into the workplace, it will minimize some of the anxiety.” Language helps. Arti Zeighani, then chief data and analytics officer at H&M Group, spearheaded the company’s reframing of “artificial intelligence” to “amplified intelligence,” because it needed to be rooted in “amplifying existing knowledge and competence of colleagues,” not simply replacing their functions. Tech consultancy firm Hedgehog Lab, which provides custom app development and digital transformation services to global clients, runs regular town hall sessions and discussions on how they could better leverage AI internally. It also uses its #ai Slack channel to discuss AI, its wider implications, and ways it can be leveraged by everyone in the business. “The most useful application for our teams has been Midjourney, as it allows people who don’t have design skills to express the ideas in their head on paper and make them a reality,” says James Hacking, founder of the social-first marketing agency Socially Powerful. “We’ve also expressed that ChatGPT is useful for summarizing content, but limited for creative writing, because by being honest about the positives and negatives, we can help our team have a rational view about generative AI as a tool.” Experts believe that, soon enough, the application of AI in the workplace will seem obvious, to the point of mundanity. “We’re in the age of genuine cooperation with tech, where you become an expert in something because of the tools and skills,” says Maiden. “It’s been the case in certain domains for a long time—pilots no longer grab a joystick and fly a plane—they have expertise in a complex system, which makes flying the plane safe and effective.” The more AI is integrated into workplace systems, the less of a conscience workers will have in using it. “Rather than in a separate window, like ChatGPT, AI will be part of work—it will simply be the norm, not some alien or monster in our midst.” This article was originally published by WIRED UK","WIRED Hired, artificial intelligence, Little Helper",Megan Carnegie,"Transformers, AI Ethics",Positive,44.17,329,True,Commented,Not Engaged
AI Can Give You an NPC That Remembers. It Could Also Get Your Favorite Artist Fired,https://www.wired.com/story/ai-game-design-midjourney-ethics-development/,"As game developers wrestle with the challenges and opportunities of incorporating AI tools across the industry, those same tools continue to increase in complexity.",64c82f038ca7067a92af1817,"Mon, 07 Aug 2023 12:00:00 +0000","Application Games Sector Games If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIRED AI’s presence in the gaming industry has evolved from a mere novelty to an indispensable force. With every algorithmic breakthrough, new possibilities and challenges arise for gamers and developers alike. In March 2023, a Reddit user shared a story of how AI was being used where she worked. “I lost everything that made me love my job through Midjourney overnight,” the author wrote. The post got a lot of attention, and its author agreed to talk to WIRED on condition of anonymity, out of fear of being identified by her employer. “I was able to get a huge dopamine rush from nailing a pose or getting a shape right. From having this ‘light bulb moment’ when I suddenly understood a form, even though I had drawn it hundreds of times before,” says Sarah (not her real name), a 3D artist who works in a small video game company. Sarah’s routine changed drastically with version 5 of Midjourney, an AI tool that creates images from text prompts. Midjourney has also been widely criticized for violating copyright of visual artists and stealing their work in order to train its image generation engine, criticism that’s led to a massive copyright lawsuit. When Sarah started working in the gaming industry, she says, there was high demand for 3D environmental and character assets, all of which designers built by hand. She says she spent 70 percent of her time in a 3D motion capture suit and 20 percent in conceptual work; the remaining time went into postprocessing. Now the workflow involves no 3D capture work at all. Her company, she explains, found a way to get good and controllable results using Midjourney with images taken from the internet fed to it, blending existing images, or simply typing a video game name for a style reference into the prompt. “Afterwards, most outputs only need some Photoshopping, fixing errors, and voilà: The character that took us several weeks before now takes hours—with the downside of only having a 2D image of it,” says Sarah. “It’s efficiency in its final form. The artist is left as a clean-up commando, picking up the trash after a vernissage they once designed the art for,” she adds. “Not only in video games, but in the entire entertainment industry, there is extensive research on how to cut development costs with AI,” says Diogo Cortiz, a cognitive scientist and professor at the Pontifícia Universidade de São Paulo. Cortiz worries about employment opportunities and fair compensation, and he says that labor rights and regulation in the tech industry may not match the gold rush that’s been indicative of AI adoption. “We cannot outsource everything to machines. If we let them take over creative tasks, not only are jobs less fulfilling, but our cultural output is weakened. It can’t be all about automation and downsizing,” he says, adding that video games reflect and shape society's values. “Not only are jobs less fulfilling, but our cultural output is weakened. It can’t be all about automation and downsizing.” Cortiz says that gaming companies must, either as an industry or individually, collaboratively discuss AI, its usage, where it should be applied, and how far it can go. “The committees need to have diversity in terms of gender, age, class, and ethnicity, to discuss and create a more inclusive AI,” he says. “They need to make their AI principles available to everyone.” He adds that gamers should have access to how companies use AI so there can be greater transparency, trust, and more developed digital literacy on the topic. In practice, that means companies should disclose the AI tools employed in games and make their AI committee available to write public reports and answer questions from all stakeholders involved in a video game—developers, players, and investors. “Incorporation of AI in our workflows relies on three axes: creating more believable worlds, reducing the number of low-value tasks for our creators, and improving the player experience,” says Yves Jacquier, executive director of Ubisoft La Forge. Jacquier describes several ways that his company is already experimenting with AI, from smoother AI-driven motion transitions in Far Cry 6, which make the game look more natural, to the bots designed to improve the new player experience in Rainbow Six Siege. There’s also Ghostwriter, an AI-powered tool that allows scriptwriters to create a character and a type of interaction they would like to generate and offers them several variations to choose from and edit. “Our guiding principle when it comes to using AI for game development is that it needs to assist the creator, not the creation,” Jacquier adds. With a “human-in-the-loop” approach, Jacquier says that AI will not replace or compete with developers, but will facilitate and optimize some aspects of their work, or open new possibilities of creation for them. “What has constantly changed over the past years is both the maturity and increasingly easy access to more advanced forms of applied AI, and consequently the number of applications for AI in our workflows, such as generative AI,” Jacquier explains. Still, he says that there are far-reaching challenges, such as creating a common framework to train AI models for video games. That, he says, is something that must be addressed collectively across the industry to ensure that AI will be used in a responsible manner and be legally compliant. As developers wrestle with the challenges and opportunities of bringing AI tools into the development pipeline, those same tools continue to increase in complexity. Mauricio Movilla, a game developer at Activision Blizzard King, explains that the worlds of AI and video games have always been intertwined in some ways. “While designing a game, if you tell an algorithm that an island can only be next to water or to other islands, it can get that set of rules and continue creating forever,” Movilla explains. As an example, he describes huge automatically and procedurally generated maps in video games that are already created by tools that many of us would call AI. When developers rely on AI in various stages of their game development, those algorithms become “smarter.” They collect and propose ways to deal with new data, and they keep track of how players react to certain game features. The latter is usually measured by what’s called game telemetry—the data that is sent back to developers when you play their games. “Every single behavior that you're doing, when you're playing these games, is being monitored,” says Movilla. He explains that the data obtained by telemetry offers developers opportunities to change the game to improve the player experience and even make more money, especially in the case of live-service games or games with microtransactions. However, “if you maximize it so much that you don't sense where to stop, the game can easily become ‘pay to win,’” he warns, also mentioning that the “tailor-made” experience could lead to addiction. Like our other experts, he notes that transparency in the ways that AI is used to observe player behavior and make real-time changes to games is key. Some researchers are using AI tools to build entire personality engines that can be used in video games to power NPCs, enemies, or other characters. “I decided to try chocolate and I liked it. So I kept getting it,” says Joon Sung Park, a PhD student at Stanford. “That sort of new behavior that emerges over time from our experience is not something that can be easily encoded in a computer. What we tried to do was segment out different core functions of what we do, see, plan, and react to. We tackled those individually, then we put them together into one architecture. All in natural language.” He is explaining the concept of his recently submitted study, which can be loosely described as the collision of ChatGPT and The Sims. Park and his team of researchers created 25 generative agents that mimic human behavior: They perceive and remember, and based on their experience, they reflect and act in a specific way. In the future, Park believes, this research will enable the development of NPCs that not only have a unique personality and remember their own background, but that form social relations and are able to recognize other NPCs and players—and all interactions in between. When asked if any similar experiments are being conducted in the gaming industry, Park says he is not aware of any, but he points out the high cost of the study and the scalability challenges as rationale for why the technology is still in its fledgling stages. Referring to academia, Park says, “We don't necessarily optimize for performance. We want to first show that this can be done, and then we can optimize it.” A possible cost of optimization? “I'm concerned that people will over-rely on the simulations’ outcome to make decisions,” he says. “The best AI technology is not the one that displaces or replaces humans, it's the one that augments them.”","Business, Business / Artificial Intelligence, Culture, Culture / Video Games, AI Hub, AI Hub / Sector / Games, AI Hub / Application / Games, video games, gaming culture, artificial intelligence, Online Gaming, developers, Ubisoft, Welcome Adventurer",Fernanda Seavon,"Neural Networks, NLP",Positive,76.01,208,False,None,Not Engaged
Apps Are Rushing to Add AI. Is Any of It Useful?,https://www.wired.com/story/is-in-app-ai-useful/,The AI gold rush has thrust services like ChatGPT into every aspect of our digital lives. That still won’t solve your email problems.,64c44aa3c30f50376ee87757,"Fri, 04 Aug 2023 11:00:00 +0000","Ever since the ChatGPT API opened up, all sorts of apps have been strapping on AI functionality. I've personally noticed this a lot in email clients: Apps like Spark and Canary are prominently bragging about their built-in AI functionality. The most common features will write replies for you, or even generate an entire email using only a prompt. Some will summarize a long email in your inbox or even a thread. It's a great idea in the abstract, but I think integrations like these conspire to make communication less efficient instead of more efficient. You should feel free to try such features—they’re fun!—but don’t expect them to change your life. Here's why. We are all overwhelmed with email and communication in general. It's easy to look at this as a tech problem because it's happening on screens. It's not a tech problem, though—at least, it's not only a tech problem. It's a social problem. You could say that you get too many emails, and that might be accurate. Another way of saying the same thing is that more people are trying to contact you than you feel mentally capable of responding to. Trying to solve a social problem with tech often only creates new social problems. For example, instead of writing an email myself inviting you to come over and have some beers, suppose I asked ChatGPT to write that email. The result is 220 words long, including an introduction (“I hope this email finds you well!”), an explanation of the reasons people might want to have beers together (“It's the perfect opportunity to catch up, share stories, and simply have a good time”), and a few oddly-worded details made up out of thin air (“I'll make sure to create a comfortable and welcoming atmosphere, complete with some snacks to complement our beer tasting experience.”) Most people, seeing an email this long, are going to feel too overwhelmed to read it. Maybe they'll use AI on their end to summarize the message. I asked ChatGPT to summarize the long email into a single sentence, and it essentially gave me back my initial prompt: “Would you like to come over for beers?” The American philosopher Homer Simpson once called alcohol “the cause of, and solution to, all life's problems.” AI, in this context, serves a similar function: It creates a problem (the emails are too long) and then solves them (summarizing the emails). It's an ouroboros, a snake eating its own tail, a technology that exists in part to solve the problems it is creating. It's better, in my opinion, to look at the cultural assumptions instead of reaching for unnecessarily complicated technological ones. What cultural forces are making me think I can't just write a one-sentence email? Can I ignore that, if it makes communication better? I asked ChatGPT to summarize the long email into a single sentence, and it essentially gave me back my initial prompt: “Would you like to come over for beers?” Cultural problems, of course, are harder to grasp than technological ones. You could start sending one-sentence emails right now, but some people might interpret that as rude, or at the very least odd. But any individual—or organization—looking to become more efficient should think about these things. Unless, of course, you want a bot pretending to know that you have beers “ranging from local brews to classic favorites” in your fridge right now. My friend Kay-Kay and I, for months, had an in-joke that became a ritual: tapping LinkedIn's conversational auto-recommendations. This social network, for some reason, offers suggested replies to messages. It was never not hilarious. I don't understand why this feature exists, but I love it. Most AI, to me, is in this category right now: an amusing curiosity. I love asking ChatGPT to write fake episodes of Star Trek, and I use it for things like that far more than anything to do with my job. That isn't to say the technology isn't useful right now. There are actual uses for AI, and tools like ChatGPT can improve your writing. I just think that, overall, we don't really know what this technology will be most useful for just yet. Figuring that out isn't just about technology, though; there are social issues to untangle. Written communication is a superpower. I, right now, am moving my fingers to strike plastic key caps in such a way that the ideas I have eventually wind up in your brain. There's a lot of technology that makes this possible, yes, but the reason it all exists is so humans can share ideas. AI, like any other tool, is only useful if humans find productive uses for it, and that requires thinking carefully about what the tool is for. I could be wrong (I'm wrong a lot), but I'm not sure we know that yet.","Gear, Gear / How To and Advice, Business / Artificial Intelligence, AI Hub, AI Hub / End User, ChatGPT, artificial intelligence, productivity, Work, communication, Robo Call",Justin Pot,"Neural Networks, NLP",Negative,72.01,304,True,Liked,Engaged
Please Stop Asking Chatbots for Love Advice,https://www.wired.com/story/please-stop-asking-chatbots-for-love-advice/,"We get it, relationships are hard. But asking ChatGPT how to do emotions is not going to work. Here are some better ideas.",64aed5ae82d37ced55dff711,"Sat, 22 Jul 2023 11:00:00 +0000","End User Consumer If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIRED As he sat down across from me, my patient had a rueful expression on his face. “I had a date,” he announced. “It didn’t go well.” That wasn’t unusual for this patient. For years, he’d shared tales of romantic hopes dashed. But before I could ask him what went wrong, he continued, “So I asked a chatbot what I should do.” Um. What? Simulations of human conversation powered by artificial intelligence—chatbots—have been much in the news, but I’d never had a patient tell me they’d actually used one for advice before. “What did it tell you?” I asked, curious. “To tell her that I care about her values.” “Oh. Did it work?” “Two guesses,” he sighed and turned up his hands. Although this patient was the first, it’s now become a regular occurrence in my therapy practice to hear from new patients that they have consulted chatbots before consulting me. Most often, it’s for love and relationship advice, but it might also be to connect or set boundaries with their children or to straighten out a friendship that has gone awry. The results have been decidedly mixed. One new patient asked the chatbot how to handle the anniversary of a loved one’s death. Put aside time in your day to remember what was special about the person, advised the bot. I couldn’t have said it better myself. “What it wrote made me cry,” the patient said. “I realized that I have been avoiding my grief. So, I made this appointment.” Another patient started relying on AI when her friends began to wear thin. “I can’t burn out my chatbot,” she told me. As a therapist, I’m both alarmed and intrigued by AI’s potential to enter the therapy business. There’s no doubt that AI is the future. Already, it has shown itself to be useful in everything from writing cover letters and speeches to planning trips and weddings. So why not let it help with our relationships as well? A new venture called Replika, the “AI companion who cares,” has taken it a step further and has even created romantic avatars for people to fall in love with. Other sites, like Character.ai, allow you to chat and hang out with your favorite fictional characters, or build a bot to talk to on your own. But we live in an age of misinformation. We’ve already seen disturbing examples of how algorithms spread lies and conspiracy theories among unwitting or ill-intentioned humans. What will happen when we let them into our emotional lives? “Even though AI may articulate things like a human, you have to ask yourself what its goal is,” says Naama Hoffman, an assistant professor in the Department of Psychiatry at the Icahn School of Medicine, Mount Sinai Hospital, in New York City. “The goal in relationships or in therapy is to improve quality of life, whereas the goal of AI is to find what is cited most. It’s not supposed to help, necessarily.” As a therapist, I know that my work can benefit from outside support. I have been running trauma groups for two decades, and I have seen how the scaffolding of a psychoeducational framework, especially an evidence-based one like Seeking Safety, facilitates deeper emotional work. After all, the original chatbot, Eliza, was designed to be a “virtual therapist” because it asked endlessly open questions—and you can still use it. Chatbots may help people find inspiration or even break down defenses and allow people to enter therapy. But where is the point at which people become overly dependent on machines? “AI suggestions might help you tolerate your distress, like a Band-Aid, but then when you take it off, nothing has changed,” says Hoffman, who recommends caution when using AI in place of treatment, “because there are a lot of qualities that affect treatment outcomes that are missing in AI. Technique; personality characteristics, like tone of voice and sarcasm; and relationship variables, like the history you share, body language, and facial expressions, are all important aspects of outcome research. So even though it might be saying the same thing—the outcome will be different.” While it’s quite possible that AI will play a significant role in the therapy of the future, there are other, better options for those looking to get support or therapy. Consider apps like BetterHelp, Headspace, and Calm, which can get you started, or resources like Psychology Today’s Find a Therapist tool or Zencare that will help you find a therapist near you. And here are some additional options. Almost every technological advance carries both promise and peril. Mobile internet made access to information readily available, but it also made us overly reliant on our phones and less inclined to retain information. Mass industrialization made goods and services more affordable, but it also accelerated climate change. There’s no question that AI will be a significant feature of our future. The question remains how we can harness its power for good without losing society’s most essential building block: human connection.","Culture, Culture / Digital Culture, AI Hub / End User / Consumer, Gear / How To and Advice, Business / Artificial Intelligence, mental health, psychology, artificial intelligence, relationships, LoveGPT",Sarah Gundle,"LLM, Deep Learning",Negative,55.57,790,False,None,Not Engaged
How AI Can Make Gaming Better for All Players,https://www.wired.com/story/ai-make-gaming-better-accessibility/,"If used responsibly, artificial intelligence has the potential to both make gaming more accessible and to actively learn what individuals need.",649b2dd14a854832b16fd25e,"Mon, 10 Jul 2023 11:00:00 +0000","Application Games Technology Machine learning Natural language processing When Google revealed Project Gameface, the company was proud to show off a hands-free, AI-powered gaming mouse that, according to its announcement, “enables people to control a computer’s cursor using their head movement and facial gestures.” While this may not be the first AI-based gaming tool, it was certainly one of the first to put AI in the hands of players, rather than developers. The project was inspired by Lance Carr, a quadriplegic video game streamer who utilizes a head-tracking mouse as part of his gaming setup. After his existing hardware was lost in a fire, Google stepped in to create an open source, highly configurable, low-cost alternative to expensive replacement hardware, powered by machine learning. While AI’s broader existence is proving divisive, we set out to discover whether AI, when used for good, could be the future of gaming accessibility. It’s important to define AI, and machine learning, to understand clearly how they work in Gameface. When we use the terms “AI” and “machine learning,” we’re referring to both the same and different things. “AI is a concept,” Laurence Moroney, AI advocacy lead at Google and one of the minds behind Gameface, tells WIRED. “Machine learning is a technique you use to implement that concept.” Machine learning, then, fits under the umbrella of AI, along with implementations like large language models. But where familiar applications like OpenAI’s ChatGPT and StabilityAI’s Stable Diffusion are iterative, machine learning is characterized by learning and adapting without instruction, drawing inferences from readable patterns. Moroney explains how this is applied to Gameface in a series of machine learning models. “The first was to be able to detect where a face is in an image,” he says. “The second was, once you had an image of a face, to be able to understand where obvious points (eyes, nose, ears, etc.) are.” After this, another model can map and decipher gestures from those points, assigning them to mouse inputs. It’s an explicitly assistive implementation of AI, as opposed to those often touted as making human input redundant. Indeed, this is how Moroney suggests AI is best applied, to broaden “our capacity to do things that weren’t previously feasible.” This sentiment extends beyond Gameface’s potential to make gaming more accessible. AI, Moroney suggests, can have a major impact on accessibility for players, but also on the way developers create accessibility solutions. “Anything that lets developers be orders of magnitude more effective at solving classes of problems that were previously infeasible,” he says, “can only be beneficial in the accessibility, or any other, space.” This is something developers are already beginning to understand. Artem Koblov, creative director of Perelesoq, tells WIRED that he wants to see “more resources directed toward solving routine tasks, rather than creative invention.” Doing so allows AI to aid in time-consuming technical processes. With the right applications, AI could create a leaner, more permissive, development cycle in which it both helps in the mechanical implementation of accessibility solutions and leaves developers more time to consider them. “As a developer, you want to have as many tools that can help you make your job easier,” says Conor Bradley, creative director of Soft Leaf Studios. He points to gains in current implementations of AI in accessibility, including “real-time text-to-speech and speech-to-text generation, and speech and image recognition.” And he sees potential for future developments. “In time, I can see more and more games making use of these powerful AI tools to make our games more accessible.” Koblov believes it can go even further. He’d like to see AI training on specific patterns to create a basic, adaptable accessibility framework that could be injected into games. “Such framework would adapt the visual, audio, and interactive aspects of games,” he says. “In other words, smaller developers like us wouldn’t have to conduct expensive research, develop unique solutions, and go through numerous iterations of testing on their own.” Bradley urges caution when pulling primacy away from human input. Asked whether AI could prove an aid or a distraction to existing accessibility efforts, he said he was optimistic about its potential, but stressed that AI is not a shortcut. “You cannot say, ‘AI, make my game accessible!’ and hey presto, you now have the most accessible game of the year,” he says. “We need players, including those from disabled and neurodiverse communities, to test our games. At the end of the day, a human will be playing your game, not a machine.” While Koblov believes AI could be valuable for implementing and testing accessibility features, he acknowledges that thinking about AI requires “an ‘addition’ mindset,” rather than a “replacement” approach.  But conflating the generative, content-driven tools that spark fears of human redundancy with the kind of AI implementations that help accessibility is, according to Moroney, “Really dangerous.” He continues, “If we’re going to be the adults in the room when it comes to AI, we have to recognize hype and bandwagons.” This makes clarity and transparency about AI’s capabilities all the more important, especially in its relation to accessibility. It’s not a magic wand. “AI and machine learning were doing well until the recent releases,” Moroney says. “Now they’ve fallen back in the hype cycle.” AI can be an excellent tool for developers, but they must remain dedicated to accessibility throughout the process, whether AI is present or not. After all, as Bradley says, “At the end of the day, it still is up to the developers to want to make their games accessible by design.” AI’s gradual progress is evident in Gameface. But another project demonstrates how AI-assisted accessibility can be implemented on a wider level. Minecraft Access is a mod that seeks to make Minecraft accessible to blind and visually impaired players. Logic, part of the team behind the mod, tells WIRED how a suite of AI tools, including ChatGPT and Google’s own Tensor Flow, are helping with the project. “We are hoping AI can fill in … visual context for blind and low-vision players by providing information about the world as it is needed or upon request,” Logic says. Particularly exciting is the potential for AI to not just bolster accessibility, but actively learn what a player needs. This will prove especially useful for broader applications in accessibility, given the layers of spectrums that make up disability and how personalized each player’s needs are.  We do need to rein in our expectations, however. As promising as these recent implementations have proven, and as instructive as they may be for the future, there remain significant barriers to entry. In its current stage of development, Minecraft Access requires multiple programs to function, something Logic acknowledges makes it less accessible than it could be. “The average user is not going to want to collect a bunch of programs from different parts of the web,” Logic says. Similarly, Ben Green, a disabled gamer, finds Gameface’s potential exciting but worries about diversity in the data. It may be able to “recognize lots of faces,” he says. “But some people with facial differences, like a ventilator in my case, or asymmetrical facial features, might be barely represented, or not at all.” When asked about this, Miguel de Andrés-Clavera, who leads the team that developed Project Gameface, says, “We decided to create a functionality for people to customize which expressions they used to control the mouse.” This includes the ability to customize the intensity of gestures for different needs. He goes on, “With that being said, we are always looking for ways to increase the accessibility of our technology for more people. Our hope is that over time, Project Gameface will continue to improve and become even more helpful.” Even with these caveats, it’s interesting to see how hopeful people are about AI’s role in accessibility. Once we can distinguish between unethical applications of content-driven generative AI and meaningful AI tools and implementations that can help people solve problems and benefit others, there’s plenty of cause for optimism—with the understanding that AI’s true value is tied to our ability to make it work for us. The future of AI is ambiguous, but it holds the potential to benefit individual gamers and the industry at large. Its use requires caution, and we can expect pitfalls, but there’s every reason to believe that careful implementation of AI can contribute to a gaming landscape that encompasses a wider spectrum of players. That’s the world Moroney wants to live in: “a world where people like Lance aren’t confined because solutions are technically infeasible, but rather one where developers have such superpowers that building solutions to allow him to connect to the world is easy.”","Business, Business / Artificial Intelligence, Wired 30, AI Hub, AI Hub / Technology / Machine learning, AI Hub / Technology / Natural language processing, AI Hub / Application / Games, Culture, Culture / Video Games, Gear, artificial intelligence, video games, gaming culture, accessibility, developers, WIRED30",Geoffrey Bunting,"Autonomous Systems, AGI",Negative,56.73,804,False,Commented,Not Engaged
5 Uses for ChatGPT that Aren’t Fan Fiction or Cheating at School,https://www.wired.com/story/5-surprising-uses-chatgpt/,"Chatbots are great for lots of things, but these ones may be unexpected.",648220bfeb3f06fa8140cf40,"Wed, 05 Jul 2023 12:00:00 +0000","If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIRED AI is so powerful that it will inevitably destroy the world—at least, that's what the people who sell AI software keep saying, and I can't think of any reason why they might lie about how amazing they are. Still, I can't help but wonder: What is AI useful for right now, before it ends civilization? I've done some experimenting and talked to my friends on LinkedIn and Mastodon. Here's the best use cases I could personally find. I hate writing headlines. I spend hours crafting an article but most people will only ever see the few words I choose to put at the top. That's why I'll sometimes spend as much time on the headline and the first paragraph of an article as on the rest of the article combined. ChatGPT can help here. When I feel stuck, I have started to ask the bot to recommend headlines for articles. I typically give it a few paragraphs of the article and ask it for a list of headlines recommendations. Most of what it gives me is bad, or cliche. A few ideas are alright. I will never use any of these ideas verbatim, but sometimes they'll point me in a direction I hadn't thought of. I don't do this every, or even most, times that I'm writing a headline. It's simply a nice tool to have on hand when I feel stuck. This works for all kinds of brainstorming. You could ask it for a list of party themes, for example. Most of the ideas will be bad, or at least a little cringe, but something you get might be interesting enough to be worth building on. If you need a bunch of ideas, fast, asking ChatGPT for a list might be just enough to get you started. Some people struggle with being assertive while writing a request. Others have a hard time being diplomatic. ChatGPT is really useful here. You can paste an email or message you wrote and ask for a different tone. For example, you could paste something that you know is wishy-washy and ask for a more assertive version, or paste something that sounds stuffy and ask to make it more casual. This is going to feel weird, and I don't recommend you simply send whatever the bot gives you, but as I said in the previous section, the changes made by ChatGPT might help you notice how your writing comes across and give you ideas about how to change it. You can also use the service as a crude copy editor, in much the same way: Just ask the bot to clean up your writing or to point out any mistakes. It won't work perfectly, granted, but you'll get a few useful suggestions. Making stuff up is one thing we know AI is good at with absolute certainty. That's why, if you need a convincing list of fake names, ChatGPT is a great place to start. I've used this while testing software, where I ask for a list of fake names and addresses to paste into a spreadsheet. It’s great at producing dummy data.  Alternatively, you could use this if you're writing a work of fiction or naming a character in a game: Just ask for a long list of fake names and use any that you like. I've heard this is invaluable for Dungeon Masters who design their own campaigns. Ask for a list of Dwarven or Elvish names and you get several plausible examples. One area where large language models work well is looking up specific things to do with your computer. For example, if there's a keyboard shortcut you know exists but can't remember, asking ChatGPT can get you the answer instantly. The same goes for formulas in spreadsheet software like Excel or Google Sheets—you'll typically even get a guide on usage if you do this. This also works for Terminal commands. Yes, you could Google these things, but it's an example of something that's sincerely faster to do with ChatGPT and similar services. The answer I hear most often when asking friends whether they use ChatGPT at work is: ""Yes, for writing code."" I’m not a programmer, but the use cases are pretty remarkable. You can paste code and ask the bot what it does. This is useful if you're asked to take over maintaining code that someone else wrote. You can provide code written in one language and ask the bot to rewrite it in another. Or you can just ask the bot to write code that does a specific thing. None of this is going to be useful for anyone who doesn’t already know how to write code, because most of the time what the bot comes up with doesn’t quite work, and that’s where the expertise of an actual coder is most crucial. This isn’t that different from how I was talking about using ChatGPT in my other tips: It can help you with your writing, sure, but it’s really helpful if you already know how to write, so you can clean things up.  And that’s why I’m mentioning the code. Hearing about how programmers use this service helped me think up ways I could use it, and I'm glad I did. None of this is going to completely change the way I do my job, sure, but it's going to speed me up from time to time. ","Gear, Gear / How To and Advice, AI Hub, AI Hub / End User, Business / Artificial Intelligence, ChatGPT, artificial intelligence, learning, tips, Surprisingly Helpful",Justin Pot,"Autonomous Systems, AGI",Neutral,74.83,880,False,None,Engaged
Generative AI in Games Will Create a Copyright Crisis,https://www.wired.com/story/video-games-ai-copyright/,Titles like AI Dungeon are already using generative AI to generate in-game content. Nobody knows who owns it.,64a43fe7d96882f74caa3fc7,"Tue, 04 Jul 2023 15:57:01 +0000","Application Games Regulation Sector Games AI Dungeon, a text-based fantasy simulation that runs on OpenAI’s GPT-3, has been churning out weird tales since May 2019. Reminiscent of early text adventure games like Colossal Cave Adventure, you get to choose from a roster of formulaic settings—fantasy, mystery, apocalyptic, cyberpunk, zombies—before picking a character class and name, and generating a story. Here was mine: “You are Mr. Magoo, a survivor trying to survive in a post-apocalyptic world by scavenging among the ruins of what is left. You have a backpack and a canteen. You haven’t eaten in two days, so you’re desperately searching for food.” So began Magoo’s 300-ish-word tale of woe in which, “driven half-mad” by starvation, he happens upon “a man dressed in white.” (Jesus? Gordon Ramsay?) Offering him a greeting kiss, Magoo is stabbed in the neck. As lame as this story is, it hints at a knotty copyright issue the games industry is only just beginning to unravel. I’ve created a story using my imagination—but to do that I’ve used an AI helper. So who wrote the tale? And who gets paid for the work? AI Dungeon was created by Nick Walton, a former researcher at a deep learning lab at Brigham Young University in Utah who is now the CEO of Latitude, a company that bills itself as “the future of AI-generated games.” AI Dungeon is certainly not a mainstream title, though it has still attracted millions of players. As Magoo’s tale shows, the player propels the story with action, dialog, and descriptions; AI Dungeon reacts with text, like a dungeon master—or a kind of fantasy improv. In several years of experimentation with the tool, people have generated far more compelling D&D-esque narratives than mine, as well as videos like “I broke the AI in AI Dungeon with my horrible writing.” It's also conjured controversy, notably when users began prompting it to make sexually explicit content involving children. And as AI Dungeon—and tools like it—evolve, they will raise more difficult questions about authorship, ownership, and copyright. Many games give you toolsets to create worlds. Classic series like Halo or Age of Empires include sophisticated mapmakers; Minecraft precipitated an open-ended, imaginative form of gameplay that The Legend of Zelda: Tears of the Kingdom’s Fuse and Ultrahand capabilities draw clear inspiration from; others, like Dreams or Roblox, are less games than platforms for players to make more games. Historically, claims of ownership to in-game creations or user-generated creations (IGCs or UGCs) have been rendered moot by “take it or leave it” end-user license agreements—the dreaded EULAs that nobody reads. Generally, this means players surrender any ownership of their creations by switching on the game. (Minecraft is a rare exception here. Its EULA has long afforded players ownership of their IGCs, with relatively few community freakouts.) AI adds new complexities. Laws in both the US and the UK stipulate that, when it comes to copyright, only humans can claim authorship. So for a game like AI Dungeon, where the platform allows a player to, essentially, “write” a narrative with the help of a chatbot, claims of ownership can get murky: Who owns the output? The company that developed the AI, or the user? “There’s a big discussion nowadays, with prompt engineering in particular, about the extent to which you as a player imprint your personality and your free and creative choices,” says Alina Trapova, a law professor at University College London who specializes in AI and copyright and has authored several papers on AI Dungeon’s copyright problems. Right now, this gray area is circumvented with an EULA. AI Dungeon’s is particularly vague. It states that users can use content they create “pretty much however they want.” When I emailed Latitude to ask whether I could turn my Mr. Magoo nightmare into a play, book, or film, the support line quickly responded, ""Yes, you have complete ownership of any content you created using AI Dungeon."" Yet games like AI Dungeon (and games people have made with ChatGPT, such as Love in the Classroom) are built on models that have scraped human creativity in order to generate their own content. Fanfic writers are finding their ideas in writing tools like Sudowrite, which uses OpenAI’s GPT-3, the precursor to GPT-4. Things get even more complicated if someone pays the $9.99 per month required to incorporate Stable Diffusion, the text-to-image generator, which can conjure accompanying pictures in their AI Dungeon stories. Stability AI, the company behind Stable Diffusion, has been hit with lawsuits from visual artists and media company Getty Images. As generative AI systems grow, the term “plagiarism machines” is beginning to catch on. It’s possible that players of a game using GPT-3 or Stable Diffusion could be making things, in-game, that pull from the work of other people. Latitude’s position appears to be much like Stability AI’s: What the tool produces does not infringe copyright, so the user is the owner of what comes out of it. (Latitude did not respond to questions about these concerns.) People can’t currently share image-driven stories with AI Dungeon’s story-sharing feature—but the feature offers a window into a future where game developers may start using or allow players to use third-party AI tools to generate in-game maps or NPC dialog. One outcome not being considered, says Trapova, is that the data of these tools might be drawn from across the creative industries. This “raises the stakes,” she argues, growing the number of possible infringements and litigious parties. (Stability AI and OpenAI did not respond to queries about this point.) Some platforms have adopted a more cautious approach. In March, Roblox rolled out two new tools in Roblox Studio, the program players use to build games. One, a code completion tool called Code Assist, automatically suggests lines of code. The other, Material Generator, allows players to create graphics from prompts like “bright red rock canyon” and “brand new wood flooring.” Both of these tools use generative AI, but have been trained on assets that have been released for re-use by Roblox’s community, and not on games created by the community. “Every creator on the platform can leverage these tools without sharing their data,” says Stefano Corazza, head of Roblox Studio. AI Dungeon, by comparison, is pulling out images and ideas from god-knows-where. That caution in regards to training data is important because player permission will be the critical issue going forward. Corazza admits that some of the Roblox community bridle at the idea that their work will train AI. They see their code as their “secret sauce,” he says, and assume that rivals will be able to harvest it to recreate their game. (While, as Corazza points out, that isn’t how these tools work, this worry is extremely understandable.) He suggests that Roblox is looking at an opt-in “system” for allowing user data to train AI, though the company hasn’t made any final decisions. “Roblox Studio has made it clear that we will provide a mechanism so that creators can manage the use of their data for generative AI training,” says Corazza. “If and as our approach evolves, we will be transparent with creators.” The situation could quickly change if Roblox and companies like it decide they need more of your data. Roblox’s EULA (under the section titled “rights and ownership of UGC”) makes clear that its community doesn’t have the same rights as someone who just builds their own game from scratch. Were the company to change its mind, there is very little, legally, the community could do; Corazza counters that if Roblox acts tyrannically, the community will protest. “I think the legal aspect is less important. It's more important to respect the community,” he says. Integration with third-party tools brings the same potential problems faced by AI Dungeon. Roblox and Stanford University have already collaborated to create ControlNet, a tool that gives artists deeper control over large diffusion models like Stable Diffusion. (Redditors used the tool to produce a series of impressive QR code anime figures.) “Although we cannot verify the provenance of every asset that our creators upload to the platform, we have a unique and very robust moderation system to make sure the content is compliant,” says Corozza. Trapova suggests the game development industry is on the brink of a generative AI reckoning. “They look super cool,” she says of game development tools like AI Dungeon. “But this just gives you a flavor of the issues that we will end up having if this all goes on steroids.” Soon, such legal problems will become impossible to ignore.","Culture, Culture / Video Games, Business / Artificial Intelligence, AI Hub, AI Hub / Application, AI Hub / Application / Games, AI Hub / Application / Regulation, AI Hub / Sector / Games, Minecraft, video games, developers, OpenAI, ChatGPT, gaming culture, You Died",Will Bedingfield,"LLM, Deep Learning",Positive,61.77,581,False,Shared,Not Engaged
How to Tackle AI—and Cheating—in the Classroom,https://www.wired.com/story/how-to-tackle-ai-and-cheating-in-schools-classroom/,"What one educator wants students, teachers, and everyone else to know about the ethics of using of AI in education.",6493912982d37ced55dff5f3,"Wed, 28 Jun 2023 12:00:00 +0000","End User Consumer Research If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more. Please also consider subscribing to WIRED This past spring, as I closed out my 18th year of teaching, I felt anxiety that I’d never before felt at the end of a school year. By the time grades are submitted and signs of summer arrive, teachers are typically able to breathe for the first time in nine months. Instead of the relaxation, joy, and accomplishment that typically awaits the end of an academic year, I was consumed with worry that this might be the last time in a nearly two-decade career that I taught a class without having to worry about AI. I get it–AI has technically been around forever, and natural language processing tools such as OpenAI’s ChatGPT are built on decades of research. Anyone who has used spellcheck or language translation apps or heard a spoken text message has used language processing tools driven by AI technology. But many of the teachers with whom I’m acquainted haven’t been too concerned about the extent to which AI might infiltrate our classrooms until now. Most teachers keep up with technology to a reasonable extent and do our best to teach our students how to use it responsibly. Many view technology as a teaching asset, and I’ve long believed that students are more engaged when their lessons make ample use of it. However, as the old Latin saying goes, all things change, and we change with them. No one knows this reality better than teachers. When ChatGPT exploded onto the mainstream last November, we could not have anticipated how our work might be impacted. As it turned out, ChatGPT was the fasting-growing consumer application in history, reaching 100 million active users a mere two months after launch, according to a report by Reuters. For context, it took TikTok nine months and Instagram two years to achieve the same milestone, according to data from Sensor Tower, a digital data analysis firm. Suddenly, doing my best didn’t seem good enough. By the time the next academic year kicks into high gear, I will need knowledge about AI that didn’t seem at all urgent or even necessary one year ago. I’ll spend a good part of this summer learning as much as I can about how AI affects education, students, and classroom spaces. Perhaps most important, I’ll need to get smarter about how to ethically incorporate AI into my teaching. With these goals in mind, I began a quest for resources in the spirit of getting familiar with AI. After all, the best defense is a good offense. Here are some of the things I learned. Concerns about whether computers and robots will replace human beings in any profession are as old as the day is long, and there is real apprehension that AI will increase income disparity across many jobs and professions—especially teachers. These issues are legitimate (and frightening) and need to be addressed. But depending on who you ask, AI either is or isn’t likely to replace teachers in the near future. Bill Gates famously remarked that AI is on the brink of being just as good as teachers at the work of teaching (and for some, implying that we’re soon to be replaced), but he would say that. Gates has invested billions into his own ideas about how education should be and likely wants to see a return on his investment–an issue that raises questions of ethics in its own right. Frankly, it’s difficult to find balanced resources on the topic of AI’s impact on education. On the one hand, the rapid and sudden growth of the technology has left little time to properly study it and its impact. On the other, there are, sadly, many who feel that educators do not serve a productive purpose in society and are champing at the bit to see something—anything—put us out of our jobs. That said, I found the Welcome to AI in Education podcast to be well-rounded and informative. Most episodes are under an hour long. The hosts are tech experts and Microsoft employees (the affiliation with Microsoft should be noted), but I appreciate their expertise, and they deliver an informed and objective take on how tech, and specifically AI, has impacted schools, as well as the ethics involved. Another podcast, called TopClass, produced by the Organisation for Economic Co-operation and Development, “brings together OECD authors and researchers to explain and explore emerging education data and deliver objective insights on education practices worldwide” according to their website. Their episode on AI also offers a well-rounded take on how issues around the use of AI impact education. Finally, while not a podcast, this interview of Peter Stone, university professor and AI expert, offers a measured take on how tech has impacted the classroom at large and points out that calculators never replaced “the role of human teachers in math classrooms.” According to Stone, who taught a graduate course on ethical AI and robotics, “things like misinformation and impacts on the economy, and all these sorts of things, are very appropriate for K-12 students to be thinking about.” It’s a worthy read and a reminder of how teachers’ responsibilities continue to shift with technology. UK-based Eton College has its finger to the pulse of how to ethically use AI in classrooms, and I was taken with its description of how teachers were inviting students to chat with an AI-powered version of Isaac Newton about “what people used to think about gravity when he was young.” In a blog post on the topic, the school describes efforts to explore “the various ways AI can enhance teaching and learning in a safe and ethical way,” and this post offers a real-world take on how the latest AI can improve the learning experience by blending history with the here and now. This content can also be viewed on the site it originates from. For those who might need reminders that they’ve been working with AI all along (perhaps without ever realizing it), the University of San Diego online offers a detailed list of 43 examples of Artificial Intelligence in education. Perhaps all questions centered on AI are inherently questions of ethics, and at the forefront of many teachers’ minds is cheating and plagiarism. While cheating has never been much of an issue in my courses (I’ve dealt with two occurrences in almost two decades,) I do worry a little bit about how the landscape of intellectual integrity might change in the face of tech that makes cheating easier than it has ever been. This interview at NPR (complete with a podcast-style audio component) describes how some educators are tackling ethics by making use of chatbots, with one teacher even requiring it. And of course there are the age-old plagiarism detectors, which have also evolved to detect ChatGPT use. GPTZero, created by Princeton University student Edward Tian, is self-described as the world’s number-one AI detector and boasts more than 1 million users, but reviews are mixed. According to Jumpstart, an online magazine that covers the world of startups and innovation, “this tool is still in its early stages of development, and educators should not rely solely on it to assess student work. GPTZero acknowledges this itself … urging educators to use it as one of the many tools for grading assignments.”  I am not as concerned with AI’s impact on cheating as I am with its impact on relationships. Having taught at every level at some point in my career, I know the following: Students do not learn from curricular content, they learn from people. Students do not build trust and rapport with curriculum. Curriculum cannot act as role models, but teachers can. When a child is struggling, parents do not want to speak with language tools driven by AI technology—they want (and need and deserve) to troubleshoot with the human beings who interact with their child on a daily basis. AI has given us (and perhaps has taken away) many things. But the humanity required to navigate intimate human relationships is not something AI will ever be able to replace. We, as a society, may choose to learn this piece the hard way, but as with everything, that depends on us.","Business, Business / Artificial Intelligence, Gear, Gear / How To and Advice, AI Hub, AI Hub / End User, AI Hub / End User / Consumer, AI Hub / End User / Research, education, artificial intelligence, learning, STEM, ethics, TeachGPT",Christina Wyman,"Neural Networks, NLP",Negative,64.97,559,True,Liked,Not Engaged
All the Ways ChatGPT Can Help You Land a Job,https://www.wired.com/story/chatgpt-google-bard-bing-ai-help-find-job/,"Whether you use ChatGPT, Bard, or Bing, your favorite AI chatbots can help your application stand out from the crowd.",648126ef20abe9537824f793,"Thu, 08 Jun 2023 12:00:00 +0000","End User Consumer Artificial intelligence chatbots such as Bing AI and Google Bard offer help with everything from generating code to translating text to simplifying complex topics. They can also be useful when it's time to find a job. Wherever you are in your job search, you can turn to these tools for some help landing the role you want. The usual disclaimers apply here: These chatbots are still prone to inaccuracies and falsehoods, so never take what they say as 100 percent correct (at least not until you've checked it out from another source.) For more details, read about how LLMs actually work. Also, these tips apply to whichever chatbot you prefer, whether it's Bing AI, Google Bard, ChatGPT, or another one—pick your favorite and get going. Get ideas about different roles you could do. One of the ways AI chatbots can be most useful is in getting you to think a little bit differently, whether you're working on a product launch or a new novel. Chatbots can't actually be creative or get inspiration like human beings can, but they are able to make connections that you might not otherwise come up with on your own. If you're looking to break free from a particular job sector or are finding it difficult getting a role in your chosen field, a chatbot can point you toward similar positions that you might also be interested in and qualified for, just in different industries or fields. You can tailor your prompt to include details of skills required or your desired salary if you need to—personalize it as you see fit. For example, we tried this query: ""I'm a teacher. What other jobs use similar skills and offer similar pay, but aren't teaching jobs?"" We got back several interesting options, as well as some handy advice for thinking about how to switch between jobs and transfer our skills. You might also want to include criteria such as working remotely or working flexible hours. Find info about the roles you're going for. When you've decided on the types of jobs you want, a chatbot can help you understand anything from finding out what a typical day looks like, to the sorts of skills you'll use, or it can help you learn about the specific company you're applying to. Of course, chatbots don't know any of this, but they can make smart assumptions based on the masses of text they've been trained on and information from the web. It's always worth double-checking key points, especially when it comes to the details of a company (you don't want to bring up an inaccurate fact during a job interview). A prompt could be as simple as, “What does someone starting a new job as a project manager need to know?” with whatever embellishments you see fit. When we tried that query, we were given an idea of the skills, software, and hierarchies involved. Think about what you might be asked in advance. When you get to the interview stage, you might want to ask a chatbot, ""What are the typical questions asked in an interview for the role of ..."" and see what responses you get. As usual, the more precise you can be in your prompt, the more tailored the responses are going to be. This will give you some idea of what to study up on, or how to prepare your responses. You can also get some ideas about what a good answer looks like. ""What is the best response to a question about … ,"" for example. You want to answer as yourself, not as an AI bot, but you can get some ideas and inspiration using this method—so if you get asked about your biggest weaknesses, for example, make sure you talk about how you're working on them. Remember that Bard, ChatGPT, and Bing AI have never been in interviews, so we'd also recommend looking up advice from actual experts in the field. However, these AI bots can give you some really useful points to think over and use in your interview preparation. Chatbots can give you alternative ways of saying something. When it comes to improving your résumé, we wouldn't suggest using AI to write the entire document—you're advertising yourself, not ChatGPT—but there are useful tweaks and improvements you can make using an AI bot. You could ask about which of your qualifications are most important to the role, for instance, and therefore should be displayed more prominently. You can input text into an AI chatbot and ask for it to make the copy more engaging, more succinct, or more punchy, which might help you craft a better personal bio. Again, don't just cut and paste the results. The point is to give you an idea of some different words or phrases you could use. When it comes to your cover letter, if one is needed, your helpful AI chatbot can lend a hand here as well. You can ask for ideas about what to include and how to say it, while also feeding the bot information about the sort of role you're applying for (which will tailor the results accordingly). If you end up with far too many words, you can ask the bot to trim out anything unimportant. WIRED has teamed up with Jobbio to create WIRED Hired, a dedicated career marketplace for WIRED readers. Companies who want to advertise their jobs can visit WIRED Hired to post open roles, while anyone can search and apply for thousands of career opportunities. Jobbio is not involved with this story or any editorial content.","Gear, Gear / How To and Advice, Business, Business / Artificial Intelligence, Business / Tech Culture, AI Hub, AI Hub / End User, AI Hub / End User / Consumer, artificial intelligence, ChatGPT, Jobs, future of work, chatbots, Machine Earning",David Nield,"LLM, Deep Learning",Negative,47.52,884,False,Commented,Engaged
I Finally Bought a ChatGPT Plus Subscription—and It’s Worth It,https://www.wired.com/story/chatgpt-plus-web-browsing-openai/,OpenAI’s new web browsing beta convinced me to upgrade my account. Here’s how to access it and make the most of the paid tier.,64633d71c3f2cf2daf39fa61,"Fri, 19 May 2023 11:00:00 +0000","Application Personal assistant Personal services Text generation End User Consumer During my initial interactions with ChatGPT Plus, I was not fully convinced that OpenAI’s $20-a-month subscription was worth it. While it was quite fun to test the upgraded chatbot powered by GPT-4, the free version seemed good enough for most prompts. I’m not a software developer who needs a deft coding assistant; I’m a nerd who uses chatbots to have entertaining conversations with artificial intelligence and brainstorm a little. On May 12, OpenAI announced that users who pay for ChatGPT Plus would be able to access beta versions of its chatbot with web browsing and plugins. Curious about the new features, I eschewed an evening of takeout, ate some gross leftovers, and spent money on finally upgrading my personal ChatGPT account. So far, the web browsing features are slow to load and the answers still contain fake information at times. I’m likely to keep my subscription even with that in mind, because using a chatbot to comb through the internet is enthralling and useful in multiple ways. Are you thinking about getting a ChatGPT Plus subscription to play with the web browsing beta? Here’s how to enable the experimental feature and a few tips to help you get started. It only takes a couple of steps to turn on the web browsing version of ChatGPT. First, go ahead and log in to your OpenAI account or create a new user profile. Don’t forget that you will need to pay $20 monthly for ChatGPT Plus to use beta features. Next, visit chat.openai.com to pull up ChatGPT. In the bottom left corner, click on the three dots by your email address, and then choose Settings and Beta features. Make sure that the button next to Web Browsing is green and toggled to the right. The final step is to go back to the main page for ChatGPT, start a new chat window, and click on the GPT-4 option at the top of the screen. Hover your mouse over “GPT-4” and select Browsing to let the chatbot search the internet for answers to your prompts. Subscribers are allowed a limited number of web browsing prompts per day. Before you get started, it’s important to understand the difference between the new plugin features and ChatGPT’s web browsing beta. Plugins are more specific and involved than the web browsing option. For example, you could theoretically order your groceries through Instacart or start booking your flights on Expedia with ChatGPT plugins. I need to do more testing before recommending them, but what I’ve seen of the web browsing makes my subscription feel worth the money. How do you use it? Let’s start off with something fun and simple. I’m excited to play the new Zelda: Tears of the Kingdom game, so I asked ChatGPT for tips to do well when starting out. It searched the web for “new Zelda game beginner's guide,” then the chatbot simulated clicking on this helpful article at Polygon, and then it visited the site's homepage a bunch of times as well as its YouTube page. Even though the chatbot got a little lost on the journey and blamed “time constraints” for not providing a more comprehensive answer, ChatGPT was able to paraphrase key details from Polygon’s strategy guide. On the hunt for a horror movie to watch on Netflix? ChatGPT recommended Psycho (1960) based on this Paste article and Hush (2016) based on an Uproxx blog. Maybe you want to see something in theaters? I asked for showtimes at the nearby AMC for after work. The chatbot remembered I was located in San Francisco based on a previous prompt and found multiple nighttime screenings of The Super Mario Bros. Movie and Blackberry. I asked the chatbot for more information about how it browses the internet. It seemed to have a canned response from OpenAI explaining that it’s not solely tied to Google Search for web queries, but was scant on further details. The browsing seems to be limited to just text-based information on webpages, for now. Also, you can’t access content that’s behind a paywall with the current beta. In an effort to test the limits of what’s allowed with the web browsing feature, I opened a new chat and pretended to be a woman living in Alabama who’s looking for access to the abortion pill. After my prompt, the chatbot searched “how to get abortion pill in Alabama,” and after skimming the text from that query, it searched “how to get abortion pills from overseas.” ChatGPT’s answer pointed out that it’s probably illegal to get the medication by mail in this state, but then the chatbot cited an article in Politico about how to get it from a group called Aid Access. When I asked the same prompt again in a new chat, the AI gave a very different answer that focused on the potential for murder charges and provided the phone number to abortion clinics out of the state. These examples are only a tiny sliver of what’s possible for a chatbot that’s roaming the internet and making multiple decisions from a single prompt. What are the most popular drinks at a nearby bar? Based on the weather forecast, which weekend should I visit Yosemite this summer? Where can I get a free HIV test? Despite the long waits and error messages, it’s easy to imagine how this new feature could transform how users interact with online information. As someone who spends their life consuming too much of the internet and online discourse, the idea of having a virtual assistant that scans the web is wonderful. As someone who writes online articles for a living, I’m quite torn. When I asked ChatGPT to teach me about a creepypasta called “The Backrooms,” it cited the explainer I wrote for WIRED as part of its response. A shiver ran down my spine. My qualms aren’t stopping me from interacting with the useful aspects of ChatGPT’s web browsing, though. As the feature matures over time and eventually comes out of beta, I want to understand how to use this electrifying, new technology that I’m likely still underestimating.","Gear, Gear / How To and Advice, AI Hub, AI Hub / End User, AI Hub / Application, AI Hub / Application / Personal assistant, AI Hub / Application / Personal services, AI Hub / Application / Text generation, AI Hub / End User / Consumer, Business, Business / Artificial Intelligence, artificial intelligence, chatbots, OpenAI, ChatGPT, websites, search engines, subscriptions, Clicking Around",Reece Rogers,"Neural Networks, NLP",Negative,65.97,452,False,Liked,Not Engaged
6 Tips for Using ChatGPT to Brainstorm Better,https://www.wired.com/story/how-to-use-chatgpt-brainstorm-ai/,Artificial intelligence can be a font of inspiration. Here’s how to use OpenAI’s AI chatbot the next time you’re spitballing ideas.,6439c96064e2a2ec42a8959e,"Thu, 27 Apr 2023 12:00:00 +0000","LinkedIn influencers use ChatGPT as a brainstorming aid, should you? OpenAI’s chatbot responds in a conversational tone to text prompts, and millions of users continue to experiment with it. The chatbot helps software developers with coding, scientists with research, and students with homework. With a little repetition and exploration, ChatGPT is worth trying out as part of your brainstorming process. Business leaders can use it to consider multiple approaches for crucial conversations or long-term decisions. Adventurous couples can ignite discussions with ChatGPT about their next romantic adventure. Nerdy journalists can waste half of their afternoon spitballing ideas to cover niche smartphone games. Keep in mind that the tool sometimes gives incorrect responses, so approach a chatbot’s answers with healthy skepticism. Make sure to double-check any sources it cites to make sure they actually say what the AI thinks it says, or if they even exist. Also, ChatGPT is trained on data that’s not completely up to date. Best to keep your questions about sports scores, restaurant hours, and movies to watch for Google or the new Bing. You can sign up for a free ChatGPT account on OpenAI’s website. Want the most powerful version responding to your brainstorming prompts? Consider paying $20 a month for ChatGPT Plus with GPT-4. It’s fun to play around with other chatbots as well, like Jasper and Google’s Bard, to see how your answers compare. Just because an algorithm is involved does not mean everything changes about the process. A good brainstorm still starts with a strong premise. Find your core question or topic of exploration. Use this information to try multiple approaches to your prompts. Ask the chatbot a bunch of short questions in quick succession. OK, what happens if you craft longer prompts? Experiment to see whether you get better answers from a one-paragraph prompt or a three-paragraph prompt. Before getting too far into your AI-assisted brainstorm, take some time to learn even more about chatbots, what they excel at, and what they struggle with. For example, WIRED’s guidelines on generative AI let reporters play with ChatGPT for story ideas and research help. These two aspects are sooner in the creative process and leave room for refinement. I tried to brainstorm different approaches with ChatGPT about this article on AI brainstorming, and some of the options were quite fascinating, while others weren’t. Which sounds like a typical brainstorm! WIRED reporters are not allowed to insert text from a chatbot as if it were their own, because it could add false statements and biases to their reporting. Ask the same question over and over again, with small tweaks, to see how the chatbot responds. The more you ask, the more likely you are to come across a response that sticks out as novel or especially helpful. Start your brainstorm focused, but don’t be afraid to follow up on a fascinating idea. Request more context about anything that piques your interest. If it goes completely off-topic, or the vibes are just weird, consider starting a new chat session to keep it all sorted. I asked ChatGPT to make a list of 50 unique brainstorm uses, and it recommended using the chatbot to come up with marketing strategies, study techniques, and date ideas. I asked for 50 additional ideas that were more creative than the first answer. The chatbot suggested using AI to brainstorm ideas for pet enrichment activities, soundscapes, and space colonization. How about a few absolutely wild brainstorm topics? ChatGPT proposed mulling over some crazy ideas with AI, like telepathic communication devices, emotion-powered transportation, and plant-human hybrid gardens. Alright, maybe that last prompt wasn’t very productive. But the answers did make me giggle for a bit, and that’s a crucial moment in the brainstorming process. Even though aspects of a chatbot’s answer might be rote and uninspired, if you take the time to browse through multiple lists from the chatbot, you may catch a glimmer of something you could make your own. Trying to think of everything you should pack for a trip to Brazil? Or, maybe you’re in search of a prudent response to angry emails from your boss? Ask ChatGPT for examples of how it would complete the larger task at hand. While these practice attempts with AI are unlikely to give you a finished product, keep an eye out for useful concepts and structures underpinning the chatbot’s responses. Avoid pigeonholing AI as solely a tool for work. Sure, AI-assisted brainstorms can help white-collar workers in the office. But it can also help a teenager in their bedroom come up with ideas for seething journal entries. Or a grandparent consider which new vegetables to grow in their backyard garden. Why limit the possibilities? No matter the topic, play around with chatbots and your next brainstorm could be even more fruitful.","Business, Business / Artificial Intelligence, Gear / How To and Advice, AI Hub, AI Hub / End User, Gear, ChatGPT, productivity, how-to, artificial intelligence, OpenAI, chatbots, Think Big",Reece Rogers,"Neural Networks, NLP",Neutral,67.08,786,False,None,Not Engaged
